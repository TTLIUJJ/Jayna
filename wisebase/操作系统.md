## 操作系统基本特征

#### 并发和并行

并发性是指宏观上在一段时间内能同时运行多个程序，而并行性则指同一时刻能运行多个指令。

并行需要硬件支持，如多流水线或者多处理器。

操作系统通过引入进程和线程，使得程序能够并发运行。

#### 共享

共享是指系统中的资源可以供多个并发进程共同使用。

有两种共享方式：互斥共享和同时共享。

互斥共享的资源称为临界资源，例如打印机等，在同一时间只允许一个进程访问，否则会出现错误，需要用同步机制来实现对临界资源的访问

#### 虚拟

虚拟技术把一个物理实体转换为多个逻辑实体。主要有两种虚拟技术：时分复用技术和空分复用技术，例如多个进程能在同一个处理器上并发执行使用了时分复用技术，让每个进程轮流占有处理器，每次只执行一小个时间片并快速切换。

#### 异步

同步：一定要等任务执行完了，得到结果，才执行下一个任务。
异步：不等任务执行完，直接执行下一个任务。

异步是指进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进。

#### 系统调用

如果一个进程在用户态需要用到操作系统的一些功能，就需要使用系统调用从而陷入内核，由操作系统代为完成。

可以由系统调用请求的功能有设备管理、文件管理、进程管理、进程通信、存储器管理等。

#### 外中断

由 CPU 执行指令以外的事件引起，如 I/O 结束中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等。

#### 异常

由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。

#### 陷入

在用户程序中使用系统调用。

#### 大内核

大内核是将操作系统功能作为一个紧密结合的整体放到内核，由于各模块共享信息，因此有很高的性能。

#### 微内核

由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。但是需要频繁地在用户态和核心态之间进行切换，会有一定的性能损失。

## 进程管理

## 进程通信

#### 进程

进程是操作系统进行资源分配的基本单位。

进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。

#### 线程

一个进程中可以有多个线程，线程是独立调度的基本单位。同一个进程中的多个线程之间可以并发执行，它们共享进程资源。

#### 区别

进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。

进程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。

#### 进程状态的切换

![](https://github.com/CyC2018/InterviewNotes/raw/master/pics//1706ce58-a081-4fed-9b36-c3c0d7e22b3a.jpg) 

阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU，缺少 CPU 会让进程从运行态转换为就绪态。

只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。

## 进程同步

#### 临界区

对临界资源进行访问的那段代码称为临界区。

为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。

```
	// entry section
	// critical section;
	// exit section	
```

#### 同步与互斥

同步指多个进程按一定顺序执行；
互斥指多个进程在同一时刻只有一个进程能进入临界区。

同步是在对临界区互斥访问的基础上，通过其它机制来实现有序访问的。

#### 信号量

信号量（Semaphore） 是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。

- down : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，将进程睡眠，等待信号量大于 0；
- up：对信号量执行 +1 操作，并且唤醒睡眠的进程，让进程完成 down 操作。

down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。

如果信号量的取值只能为 0 或者 1，那么就成为了互斥量（Mutex），0 表示临界区已经加锁，1 表示临界区解锁。

```c
typedef int  semaphore ;
semaphore mutex = 1;
void P1() {
    down(mutex);
    // 临界区
    up(mutex);
}

void P2() {
    down(mutex);
    // 临界区
    up(mutex);
}
```

使用信号量实现生产者-消费者问题

使用一个互斥量 mutex 来对临界资源进行访问；empty 记录空缓冲区的数量，full 记录满缓冲区的数量。

注意，必须先执行 down 操作再用互斥量对临界区加锁，否则会出现死锁。因为如果都先对临界区加锁，然后再执行 down 操作，那么可能会出现这种情况：生产者对临界区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生成者睡眠。消费者此时不能进入临界区，因为生产者对临界区加锁了，也就无法执行 up(empty) 操作，那么生产者和消费者就会一直等待下去。

```c
#define N 100
typedef int  semaphore ;
semaphore mutex = 1;
semaphore empty = N;
semaphore full = 0;

void producer() {
    while(TRUE){
        int item = produce_item;
        down(empty);
        down(mutex);
        insert_item(item);
        up(mutex);
        up(full);
    }
}

void consumer() {
    while(TRUE){
        down(full);
        down(mutex);
        int item = remove_item(item);
        up(mutex);
        up(empty);
        consume_item(item);
    }
}
```

## 进程通信

进程通信可以看成是不同进程间的线程通信，对于同一个进程内线程的通信方式，主要使用信号量、条件变量等同步机制。

#### 管道

管道是单向的、先进先出的、无结构的、固定大小的字节流，它把一个进程的标准输出和另一个进程的标准输入连接在一起。写进程在管道的尾端写入数据，读进程在管道的首端读出数据。数据读出后将从管道中移走，其它读进程都不能再读到这些数据。

管道提供了简单的流控制机制，进程试图读空管道时，在有数据写入管道前，进程将一直阻塞。同样地，管道已经满时，进程再试图写管道，在其它进程从管道中移走数据之前，写进程将一直阻塞。

inux 中管道是通过空文件来实现。

管道有三种：

- 普通管道：有两个限制：一是只支持半双工通信方式，即只能单向传输；二是只能在父子进程之间使用；
- 流管道：去除第一个限制，支持双向传输；
- 命名管道：去除第二个限制，可以在不相关进程之间进行通信。

#### 信号量

信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其它进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

#### 信号

信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。

#### 共享内存

共享内存就是映射一段能被其它进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其它 IPC 运行效率低而专门设计的。它往往与其它通信机制（如信号量）配合使用，来实现进程间的同步和通信。

#### 套接字

套接字也是一种进程间通信机制，与其它通信机制不同的是，它可用于不同机器间的进程通信。

## 经典同步问题

生产者和消费者问题前面已经讨论过。

#### 读者-写者问题

允许多个进程同时对数据进行读操作，但是不允许读和写以及写和写操作同时发生。

一个整型变量 count 记录在对数据进行读操作的进程数量；
一个互斥量 count_mutex 用于对 count 加锁；
一个互斥量 data_mutex 用于对读写的数据加锁。

```c
typedef int semaphore;
semaphore count_mutex = 1;
semaphore data_mutex = 1;
int count = 0;

void reader() {
    while(TRUE) {
        down(count_mutex);	
        count++;
        if(count == 1) down(data_mutex); // 第一个读者需要对数据进行加锁，防止写进程访问
        up(count_mutex);
        read();
        down(count_mutex);	//尝试下，是否可以运行写进程
        count--;
        if(count == 0) up(data_mutex);
        up(count_mutex);
    }
}

void writer() {
    while(TRUE) {
        down(data_mutex);
        write();
        up(data_mutex);
    }
}
```

#### 哲学家进餐问题

![](https://github.com/CyC2018/InterviewNotes/raw/master/pics//a9077f06-7584-4f2b-8c20-3a8e46928820.jpg) 

五个哲学家围着一张圆周，每个哲学家面前放着饭。哲学家的生活有两种交替活动：吃饭以及思考。当一个哲学家吃饭时，需要先一根一根拿起左右两边的筷子。

```
#define N 5
#define LEFT (i + N - 1) % N
#define RIGHT (i + N) % N
typedef int semaphore;
semaphore chopstick[N];

void philosopher(int i) {
    while(TURE){
        think();
        down(chopstick[LEFT[i]]);
        down(chopstick[RIGHT[i]]);
        eat();
        up(chopstick[RIGHT[i]]);
        up(chopstick[LEFT[i]]);
    }
}
```

为了防止死锁的发生，可以加一点限制，只允许同时拿起左右两边的筷子，方法是引入一个互斥量，对拿起两个筷子的那段代码加锁。

```
semaphore mutex = 1;

void philosopher(int i) {
    while(TURE){
        think();
        down(mutex);
        down(chopstick[LEFT[i]]);
        down(chopstick[RIGHT[i]]);
        up(mutex);
        eat();
        down(mutex);
        up(chopstick[RIGHT[i]]);
        up(chopstick[LEFT[i]]);
        up(mutex);
    }
}
```

## 死锁

#### 死锁的条件

![](https://github.com/CyC2018/InterviewNotes/raw/master/pics//c037c901-7eae-4e31-a1e4-9d41329e5c3e.png) 

- 互斥条件：进程对所分配到的资源不允许其他进程进行访问，若其他进程访问该资源，只能等待，直至占有该资源的进程使用完成后释放该资源
- 请求和保持条件：进程获得一定的资源之后，又对其他资源发出请求，但是该资源可能被其他进程占有，此事请求阻塞，但又对自己获得的资源保持不放
- 不可抢占条件：是指进程已获得的资源，在未完成使用之前，不可被剥夺，只能在使用完后自己释放
- 环路等待条件：是指进程发生死锁后，必然存在一个进程--资源之间的环形链 

#### 安全状态

![](https://github.com/CyC2018/InterviewNotes/raw/master/pics//ed523051-608f-4c3f-b343-383e2d194470.png)

图 a 的第二列 has 表示已拥有的资源数，第三列 max 表示总共需要的资源数，free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源，运行结束后释放 B，此时 free 变为 4；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。

定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的

#### 单个资源的银行家算法

一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。

![](https://github.com/CyC2018/InterviewNotes/raw/master/pics//d160ec2e-cfe2-4640-bda7-62f53e58b8c0.png)

上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。

#### 多个资源的银行家算法

![](https://github.com/CyC2018/InterviewNotes/raw/master/pics//62e0dd4f-44c3-43ee-bb6e-fedb9e068519.png)

上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。

检查一个状态是否安全的算法如下：
- 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。
- 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。
- 重复以上两步，直到所有进程都标记为终止，则状态时安全的。

#### 死锁检测算法

死锁检测的基本思想是，如果一个进程所请求的资源能够被满足，那么就让它执行，释放它拥有的所有资源，然后让其它能满足条件的进程执行。

![](https://github.com/CyC2018/InterviewNotes/raw/master/pics//e1eda3d5-5ec8-4708-8e25-1a04c5e11f48.png)

上图中，有三个进程四个资源，每个数据代表的含义如下：
- E 向量：资源总量
- A 向量：资源剩余量
- C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量
- R 矩阵：每个进程请求的资源数量

进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P1 可以执行，执行后释放 P1 拥有的资源，A = (4 2 2 2) ，P2 也可以执行。所有进程都可以顺利执行，没有死锁。

**算法总结：**每次都让资源剩余量满足某个进程的要求向量

## 设备管理

#### 磁盘调度算法

**先来先服务**

根据进程请求访问磁盘的先后次序来进行调度。优点是公平和简单，缺点也很明显，因为未对寻道做任何优化，使平均寻道时间可能较长。

**最短寻道时间优先**

要求访问的磁道与当前磁头所在磁道距离最近的优先进行调度。这种算法并不能保证平均寻道时间最短，但是比 先来先服务 好很多。

饥饿现象：新进程请求访问的磁道与磁头所在磁道的距离总是比一个在等待的进程来的近，那么等待的进程会一直等待下去。

**扫描算法**

要求所请求访问的磁道在磁头当前移动方向上才能够得到调度。因为考虑了移动方向，那么一个进程请求访问的磁道一定会得到调度。

当一个磁头自里向外移动时，移到最外侧会改变移动方向为自外向里，这种移动的规律类似于电梯的运行，因此又称为电梯调度算法。

**循环扫描算法**

要求磁头始终沿着一个方向移动

## 存储器管理

#### 虚拟内存

虚拟内存是计算机系统内存管理的一种技术。它使得应用程序认为它拥有连续的可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术的系统使得大型程序的编写变得更容易，对真正的物理内存（例如RAM）的使用也更有效率。

物理内存有限，是一种稀缺资源
32位系统中，每个进程独立的占有4G虚拟空间。

虚拟内存优势：
用户程序开发方便
保护内核不受恶意或者无意的破坏
隔离各个用户进程

#### 分段和分页

**分页：** 

用户程序的地址空间被划分为若干固定大小的区域，称为“页”。相应地，内存空间分成若干个物理块，页和块的大小相等。可将用户程序的任一页放在内存的任一块中，实现了离散分配，由一个页表来维护它们之间的映射关系。

**分段：**

![](https://github.com/CyC2018/InterviewNotes/raw/master/pics//22de0538-7c6e-4365-bd3b-8ce3c5900216.png) 

上图为一个编译器在编译过程中建立的多个表，有 4 个表是动态增长的，如果使用分页系统的一维地址空间，动态递增的特点会导致覆盖问题的出现。

![](https://github.com/CyC2018/InterviewNotes/raw/master/pics//e0900bb2-220a-43b7-9aa9-1d5cd55ff56e.png) 

分段的做法是把每个表分成段，一个段构成一个独立的地址空间。每个段的长度可以不同，并且可以动态增长。

每个段都需要程序员来划分。

**段页式：**

用分段方法来分配和管理虚拟存储器。程序的地址空间按逻辑单位分成基本独立的段，而每一段有自己的段名，再把每段分成固定大小的若干页。

用分页方法来分配和管理实存。即把整个主存分成与上述页大小相等的存储块，可装入作业的任何一页。

程序对内存的调入或调出是按页进行的，但它又可按段实现共享和保护。

#### 分页与分段区别

- 对程序员的透明性：分页透明，但是分段需要程序员显示划分每个段。
- 地址空间的维度：分页是一维地址空间，分段是二维的。
- 大小是否可以改变：页的大小不可变，段的大小可以动态改变。
- 出现的原因：分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护。

#### 页面置换算法

在程序运行过程中，若其所要访问的页面不在内存而需要把它们调入内存，但是内存已无空闲空间时，系统必须从内存中调出一个页面到磁盘对换区中，并且将程序所需要的页面调入内存中。页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。

**最佳（Optimal）**

所选择的被换出的页面将是最长时间内不再被访问，通常可以保证获得最低的缺页率。
是一种理论上的算法，因为无法知道一个页面多长时间会被再访问到。

**先进先出（FIFO）**

所选择换出的页面是最先进入的页面。
该算法会将那些经常被访问的页面也被换出，从而使缺页率升高。

**最近最少未使用（LRU）**

核心思想是：如果数据最近被访问过，那么将来被访问的几率也更高
最常见的实现是使用一个链表保存缓存数据，详细算法实现如下：
![](http://my.csdn.net/uploads/201205/24/1337859321_3597.png) 

- 新数据插入队头
- 每当缓存命中（即缓存数据被访问），将其移到队头
- 当页面缓存达到最大，将链表尾的数据丢弃

命中率：当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致LRU命中率急剧下降，缓存污染情况比较严重。

java中最简单的LRU算法实现，就是利用jdk的LinkedHashMap，覆写其中的removeEldestEntry(Map.Entry)方法即可。

```java
public class LRULinkedHashMap<K, V> extends LinkedHashMap<K, V> {  
    private final int maxCapacity;  
   
    private static final float DEFAULT_LOAD_FACTOR = 0.75f;  
   
    private final Lock lock = new ReentrantLock();  
   
    public LRULinkedHashMap(int maxCapacity) {  
        super(maxCapacity, DEFAULT_LOAD_FACTOR, true);  
        this.maxCapacity = maxCapacity;  
    }  
   
    @Override 
    protected boolean removeEldestEntry(java.util.Map.Entry<K, V> eldest) {  
        return size() > maxCapacity;  
    }  
    @Override 
    public boolean containsKey(Object key) {  
        try {  
            lock.lock();  
            return super.containsKey(key);  
        } finally {  
            lock.unlock();  
        }  
    }  
   
       
    @Override 
    public V get(Object key) {  
        try {  
            lock.lock();  
            return super.get(key);  
        } finally {  
            lock.unlock();  
        }  
    }  
   
    @Override 
    public V put(K key, V value) {  
        try {  
            lock.lock();  
            return super.put(key, value);  
        } finally {  
            lock.unlock();  
        }  
    }  
   
    public int size() {  
        try {  
            lock.lock();  
            return super.size();  
        } finally {  
            lock.unlock();  
        }  
    }  
   
    public void clear() {  
        try {  
            lock.lock();  
            super.clear();  
        } finally {  
            lock.unlock();  
        }  
    }  
   
    public Collection<Map.Entry<K, V>> getAll() {  
        try {  
            lock.lock();  
            return new ArrayList<Map.Entry<K, V>>(super.entrySet());  
        } finally {  
            lock.unlock();  
        }  
    }  
}  
```

**LRU-K**

LRU-K中的K代表最近使用的次数，因此LRU可以认为是LRU-1。LRU-K的主要目的是为了解决LRU算法“缓存污染”的问题，其核心思想是将“最近使用过1次”的判断标准扩展为“最近使用过K次”。

相比LRU，LRU-K需要多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到K次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K会淘汰第K次访问时间距当前时间最大的数据。详细实现如下：

![](http://my.csdn.net/uploads/201205/24/1337859332_7838.png) 

LRU-K具有LRU的优点，同时能够避免LRU的缺点，实际应用中LRU-2是综合各种因素后最优的选择，LRU-3或者更大的K值命中率会高，但适应性差，需要大量的数据访问才能将历史访问记录清除掉。

**Multi Queue（MQ）**

MQ算法根据访问频率将数据划分为多个队列，不同的队列具有不同的访问优先级，其核心思想是：优先缓存访问次数多的数据。

MQ算法将缓存划分为多个LRU队列，每个队列对应不同的访问优先级。访问优先级是根据访问次数计算出来的。
详细的算法结构图如下，Q0，Q1....Qk代表不同的优先级队列，Q-history代表从缓存中淘汰数据，但记录了数据的索引和引用次数的队列：

![](http://my.csdn.net/uploads/201205/24/1337859354_3696.png) 

