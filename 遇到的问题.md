# 遇到的问题

#### C10K问题本质？

C10K问题本质是操作系统的问题，创建的线程多了，数据频繁拷贝（I/O、内核数据和用户进程之间的数据交换、阻塞），进程或者线程上下文切换消耗大，从而导致操作系统奔溃。

#### 为什么使用Reactor模型而不是Proactor模型？

Reactor模型是当前较为流行又比较成熟的同步非阻塞模型，Proactor模型是异步非阻塞模型，二者的主要区别是同步I/O需要用户从缓冲区读取数据，然后调用函数处理数据，而异步I/O在内核取完数据之后，自动回调处理函数。

Netty的作者对AIO做过这样的评价：

- 在Unix系统下，AIO的效率并不会比NIO（epoll）快
- AIO不支持数据报
- Unnecessary threading model (too much abstraction without usage)

在Linux下，AIO的实现方式是内核和应用进程共享一片内存区域，应用检测到这个内存区域来得知fd是否有数据（避免调用NonBlocking的read和write函数来测试数据数据是否到来，这两个函数需要需要切换用户态和内核态，仍旧效率不高）。可是检测内存区并不是实时的，需要在线程中构造一个监控内存的循环，设置sleep，总的效率不如epoll这样的实时通知。

#### Selector底层实现原理

Java NIO的核心类库多路复用器Selector就是基于epoll的多路复用技术实现的，所以直接研究epoll的底层实现。

相对于select和poll系统调用，epoll有如下优点：

- 支持一个进程打开的socket描述付（FD）不受限制，仅受限于操作系统的最大句柄数。select系统最大的缺陷是单个进程所打开的FD是有一定的限制，它由FD_SETSIZE设置，默认值是1024。可以选择修改这个宏重新编译内核，但是会带来网络效率的下降；也可以选择多进程方案，但是进程创建和切换成本需要考虑。可以查看通过cat /proc/sys/fs/file-max查看，这个值和系统的内存关系较大，笔者的电脑支持34000+；

- I/O效率不会随着FD数目的增加而线性下降。当拥有一个很大的socket集合，但却只有一少部分socket是“活跃”的，select和poll每次都要遍历整个集合，效率线性下降。而epoll只会对“活跃”的socket进行操作，所以只需要遍历被内核唤醒而加入就绪集合的有I/O事件的socket；

- 使用mmap加速内核与用户空间的数据传递。mmap系统调用允许放在设备上的文件的部分信息映射到进程的部分地址空间，从而减少内核态与用户态之间的切换。 

#### HTTP连接

长连接是指在一个TCP连接上可以发送多个HTTP请求和响应，相对于短连接，可以避免多次TCP的三次握手和四次挥手。

实现长连接的注意事项：
	