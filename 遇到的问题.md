# 遇到的问题

#### C10K问题本质？

C10K问题本质是操作系统的问题，创建的线程多了，数据频繁拷贝（I/O、内核数据和用户进程之间的数据交换、阻塞），进程或者线程上下文切换消耗大，从而导致操作系统奔溃。

#### 为什么使用Reactor模型而不是Proactor模型？

Reactor模型是当前较为流行又比较成熟的同步非阻塞模型，Proactor模型是异步非阻塞模型，二者的主要区别是同步I/O需要用户从缓冲区读取数据，然后调用函数处理数据，而异步I/O在内核取完数据之后，自动回调处理函数。

Netty的作者对AIO做过这样的评价：

- 在Unix系统下，AIO的效率并不会比NIO（epoll）快
- AIO不支持数据报
- Unnecessary threading model (too much abstraction without usage)

在Linux下，AIO的实现方式是内核和应用进程共享一片内存区域，应用检测到这个内存区域来得知fd是否有数据（避免调用NonBlocking的read和write函数来测试数据数据是否到来，这两个函数需要需要切换用户态和内核态，仍旧效率不高）。可是检测内存区并不是实时的，需要在线程中构造一个监控内存的循环，设置sleep，总的效率不如epoll这样的实时通知。

#### Selector底层实现原理

可在Linux下，内核版本大于2.6时使用epoll，小于2.6时使用poll

Java NIO的核心类库多路复用器Selector就是基于epoll的多路复用技术实现的，所以直接研究epoll的底层实现。

相对于select和poll系统调用，epoll有如下优点：

- 支持一个进程打开的socket描述付（FD）不受限制，仅受限于操作系统的最大句柄数。select系统最大的缺陷是单个进程所打开的FD是有一定的限制，它由FD_SETSIZE设置，默认值是1024。可以选择修改这个宏重新编译内核，但是会带来网络效率的下降；也可以选择多进程方案，但是进程创建和切换成本需要考虑。可以查看通过cat /proc/sys/fs/file-max查看，这个值和系统的内存关系较大，笔者的电脑支持34000+；

- I/O效率不会随着FD数目的增加而线性下降。当拥有一个很大的socket集合，但却只有一少部分socket是“活跃”的，select和poll每次都要遍历整个集合，效率线性下降。而epoll只会对“活跃”的socket进行操作，所以只需要遍历被内核唤醒而加入就绪集合的有I/O事件的socket；

- 使用mmap加速内核与用户空间的数据传递。mmap系统调用允许放在设备上的文件的部分信息映射到进程的部分地址空间，从而减少内核态与用户态之间的切换。 

#### HTTP连接

长连接是指在一个TCP连接上可以发送多个HTTP请求和响应，相对于短连接，可以避免多次TCP的三次握手和四次挥手。
	

#### NIO中SocketChanel.read() == 0 

当socketChannecl为阻塞模式下，是不会出现这样的情况。要么有数据可读返回正数，要么读取完毕返回-1，要么一直阻塞等待读取数据。

非阻塞模式下：

- read 返回 -1的情况：
表明客户端的数据发送完毕，并且主动关闭socket。所以在这样情况下，服务端程序应该关闭SocketChannel取消Key。如果这个时候还调用read操作，就会抛出“远程主机强迫关闭一个现有的连接”异常

- read 返回 0 的情况：
	- 当前SocketChannel没有数据可以读，返回0
	- ByteFuffer的position等于limit，即remainng == 0，这个时候也返回0
	- 客户端数据发送完毕，而服务端继续尝试读取数据，返回0

处理返回0的情况，只需重新注册读事件

```java
	SelectionKey.register(selector, key.interestOps() | OP_READ, context)；
	//作用类似于在阻塞模式下，读取完整的数据
	while((len = inStream.read(buf)) > 0){
		// ...
	}
```

将当前读取到的任务保存在RequestMessage中，当当前通道读事件触发的时候，再次读取补充完整的数据。

另一个简单易懂的解释：

当某个socket接收缓冲区有新数据分节到达，然后select报告这个socket描述符可读，但随后协议栈检查到这个新分节检验错误，并将其丢弃，这个时候调用read函数则无数据可读，epoll也是一样的。
这也是为什么要使用非阻塞I/O的原因，就算是select和epoll。

#### 完整的请求处理过程	

- 错误断开连接：
	- 解析请求发生错误，断开连接
- 正常断开连接：
	- 执行结束且为HTTP短连接
- 重新返回循环，重新注册OP_READ：
	- 解析状态为自定义的PARSE_MORE
	- 数据未读取完
- 更新定时任务、重新注册OP_READ:
	- 执行结束且为HTTP长连接，等待下次数据的到来
	
#### EPOLLONESHOT

JaynaHTTPServer每次将I/O事件加入线程池，当线程池的任务队列过多，来不及处理事件而Selector在LT（水平触发模式）下，每次都会提醒事件可操作，这样同一个任务就会被多个线程处理，即一个Socket可能同时被多个线程处理，在epoll在使用EPOLLONESHOT避免该情况。

在Java NIO可以使用下面的代码，避免此情况：

```java
	//在任务加入线程之前，下次select不再提醒该事件
	key.interestOps(key.interestOps() & (~SelectionKey.OP_READ));
```

#### CPU接近满负荷的情况

- 问题描述
在JaynaHTTPServer工作情况下，CPU占用率飙到90左右

- 初步分析
通常CPU占有率在90%的情况下是由于死循环引起的

- 故障排查
在第一次设置关闭Socket任务的时候，伪代码如下：

```java
	while(true){
		while(true){
			SelectionKey key = priorityQueue.peek();
			MonitoredKey monitoredKey = hashMap.get(key);
			long now = new Date().getTime();
			if(now <  monitoredKey.getExpireTime()){
				break;
			}
			priorityQueue.remove(key);
			hashMap.remove(key);
			key.channel().close();
		}
	}
```

- 解决方案
使用下面的定时任务，简单高效

#### 定时任务的实现

CancenlTask实现了Callable接口，任务就是取消Socket连接。
由于长连接过程中，客户端Socket可能会再次请求，这时就要取消Socket关闭的任务，重新设置任务的时间。
使用FutrueTask可以取消加入任务，使用ConcurrentHashMap可以在高并发情况下快速找到Socket（即SelectionKey）对应的任务。

```
    public void addFutureTask(SelectionKey selectionKey){
        Callable<Void> c = new CancelTask(selectionKey);
        FutureTask<Void> futureTask = new FutureTask<Void>(c);  //任务是简单关闭Socket连接
        futureTaskMap.put(selectionKey, futureTask);    //futureTaskMap是ConcurrentHashMap类
        scheduledThreadPoolExecutor.schedule(futureTask, timeout, TimeUnit.MILLISECONDS);   //timeout是HTTP连接时长
    }
```

#### 解析请求完毕，注册OP_WRITE之后阻塞

```java
    client.register(selector, SelectionKey.OP_WRITE, context);
    // 没有这个  当前线程会被卡住
    selector.wakeup();
```

selector.wakeUp()主要是为了唤醒阻塞在selector.select()上的线程，让该线程及时去处理其他注册事情。

#### 抓包问题

使用下面的命令抓取本地换回数据

```shell
tcpdump -i lo 
```

#### HTTP状态机解析请求

 TCP是基于数据流进行数据传输的，可能由于发送端和接收端的问题造成数据包不完整，这时可以将系统当前读取到的数据情况保存到HttpContext然后attach到SocketChannel之中，,等待下次数据的到来。如果直接使用字符串解析HTTP请求，可能会造成很多不必要的错误。